# ü§ñ Guia dos Agentes e Projetos IA

Este guia documenta todos os projetos de IA, agentes e ferramentas dispon√≠veis na pasta **AGENTES**. Cada projeto tem sua pr√≥pria finalidade e forma de uso.

---

## üìã √çndice

- [üß† Modelos de IA](#-modelos-de-ia)
- [üîß Servidores MCP](#-servidores-mcp)
- [üí¨ Interfaces de Chat](#-interfaces-de-chat)
- [üõ†Ô∏è Ferramentas de Desenvolvimento](#Ô∏è-ferramentas-de-desenvolvimento)
- [üìö Tutoriais e Exemplos](#-tutoriais-e-exemplos)
- [üéØ Projetos Especializados](#-projetos-especializados)
- [üìñ Recursos e Documenta√ß√£o](#-recursos-e-documenta√ß√£o)

---

## üß† Modelos de IA

### üìÑ Llama3_2_(1B_and_3B)_Conversational.ipynb
**Notebook Jupyter para fine-tuning de modelos Llama 3.2**

- **Finalidade**: Tutorial completo para treinar modelos Llama 3.2 (1B e 3B par√¢metros) usando Unsloth
- **Como usar**:
  1. Execute no Google Colab com GPU Tesla T4 gratuita
  2. Pressione "Runtime" ‚Üí "Run all"
  3. O notebook inclui instala√ß√£o, prepara√ß√£o de dados, treinamento e infer√™ncia
- **Funcionalidades**:
  - Fine-tuning com LoRA adapters
  - Formato de conversa√ß√£o Llama-3.1
  - Dataset FineTome-100k
  - Quantiza√ß√£o 4-bit para economia de mem√≥ria

### üßÆ modelo-bitnet-b1.58-2B-4T/
**Modelo BitNet B1.58 da Microsoft - LLM nativo de 1-bit**

- **Finalidade**: Modelo de linguagem ultra-eficiente com pesos de 1.58-bit
- **Caracter√≠sticas**:
  - 2 bilh√µes de par√¢metros
  - Treinado em 4 trilh√µes de tokens
  - Mem√≥ria n√£o-embedding: apenas 0.4GB
  - Lat√™ncia CPU: 29ms por token
- **Como usar**:
  ```python
  from transformers import AutoModelForCausalLM, AutoTokenizer
  model_id = "microsoft/bitnet-b1.58-2B-4T"
  tokenizer = AutoTokenizer.from_pretrained(model_id)
  model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16)
  ```
- **‚ö†Ô∏è Importante**: Para m√°xima efici√™ncia, use `bitnet.cpp` em vez do transformers

### ü¶ô modelo-llama-cpp-python/
**Bindings Python para llama.cpp**

- **Finalidade**: Interface Python para executar modelos Llama localmente
- **Como usar**:
  ```bash
  pip install llama-cpp-python
  ```
  ```python
  from llama_cpp import Llama
  llm = Llama(model_path="./models/7B/ggml-model.bin")
  output = llm("Sua pergunta aqui", max_tokens=32)
  ```
- **Funcionalidades**:
  - API compat√≠vel com OpenAI
  - Servidor web integrado
  - Suporte LangChain
  - Acesso completo √† API C do llama.cpp

---

## üîß Servidores MCP

### üßä cryo-mcp/
**Servidor MCP para extra√ß√£o de dados blockchain**

- **Finalidade**: Acessar dados da blockchain Ethereum via protocolo MCP
- **Como usar**:
  ```bash
  # Instalar
  uv tool install cryo-mcp
  
  # Executar
  uvx cryo-mcp -r <ETH_RPC_URL>
  ```
- **Funcionalidades principais**:
  - `list_datasets()` - Lista datasets dispon√≠veis
  - `query_dataset()` - Consulta dados blockchain
  - `get_latest_ethereum_block()` - √öltimo bloco
  - Filtros por contrato, intervalo de blocos
  - Exporta√ß√£o JSON/CSV

### üêô github-mcp-server/
**Servidor MCP para automa√ß√£o GitHub**

- **Finalidade**: Integra√ß√£o completa com APIs do GitHub via MCP
- **Como usar**:
  ```bash
  # Via Docker
  docker run -i --rm -e GITHUB_PERSONAL_ACCESS_TOKEN ghcr.io/github/github-mcp-server
  ```
- **Funcionalidades**:
  - **Issues**: criar, listar, comentar, atualizar
  - **Pull Requests**: gerenciar, revisar, mesclar
  - **Reposit√≥rios**: criar, buscar, gerenciar arquivos
  - **Busca**: c√≥digo, usu√°rios, reposit√≥rios
  - **Code Scanning**: alertas de seguran√ßa
- **Requisitos**: Token de acesso pessoal do GitHub

---

## üí¨ Interfaces de Chat

### ü§ñ chatbot-deepseek-interface/
**Interface web para modelo R1 Deepseek**

- **Finalidade**: Interface web completa para interagir com modelos Deepseek
- **Como instalar**:
  ```bash
  # Depend√™ncias Python
  pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
  pip install transformers python-bridge
  
  # Depend√™ncias Node.js
  cd interface1
  pnpm install
  pnpm build
  ```
- **Como usar**:
  ```bash
  cd interface1/apps/web
  pnpm dev
  # Acesse http://localhost:3000
  ```
- **Interfaces dispon√≠veis**:
  - **Interface 1**: Design minimalista (`/interface1`)
  - **Interface 2**: Layout com hist√≥rico (`/interface2`)
- **‚ö†Ô∏è Requisitos**: 8GB RAM, modelo executa em CPU

---

## üõ†Ô∏è Ferramentas de Desenvolvimento

### ‚ö° codex/
**CLI de programa√ß√£o com IA (fork melhorado)**

- **Finalidade**: Agente de programa√ß√£o que roda no terminal
- **Como instalar**:
  ```bash
  npm install -g @ymichael/codex
  export OPENAI_API_KEY="sua-chave-aqui"
  ```
- **Como usar**:
  ```bash
  # Modo interativo
  codex
  
  # Com prompt direto
  codex "explique este c√≥digo"
  
  # Modo autom√°tico
  codex --approval-mode full-auto "crie um app todo"
  ```
- **Funcionalidades**:
  - M√∫ltiplos provedores: OpenAI, Gemini, OpenRouter, Ollama
  - Sandboxing de seguran√ßa
  - Aprova√ß√£o autom√°tica configur√°vel
  - Integra√ß√£o com Git
  - Suporte multimodal

### üîó openai-mcp-client/
**Cliente MCP para OpenAI**

- **Finalidade**: Exemplo de integra√ß√£o MCP com API OpenAI
- **Como usar**:
  ```bash
  # Instalar Deno v2
  deno install
  
  # Configurar .env
  cp .env.example .env
  # Preencher valores
  
  # Executar
  deno run dev
  ```
- **Funcionalidades**:
  - Agente conversacional com ferramentas MCP
  - Suporte apenas para respostas tipo texto
  - Debug mode dispon√≠vel
  - Mensagens salvas em `messages.json`

---

## üìö Tutoriais e Exemplos

### ü¶ô ollama_tutoriais/
**Scripts Python para usar Ollama localmente**

- **Finalidade**: Exemplos pr√°ticos de uso do Ollama
- **Como usar**:
  ```bash
  # Verificar se Ollama est√° rodando
  systemctl status ollama.service
  
  # Executar scripts
  python ollama_basico.py
  python ollama_basico_chat.py
  python ollama_basico_chat_stream.py
  python ollama_chatgpt.py
  ```
- **Scripts dispon√≠veis**:
  - `ollama_basico.py`: Pergunta simples
  - `ollama_basico_chat.py`: Chat b√°sico
  - `ollama_basico_chat_stream.py`: Chat com streaming
  - `ollama_chatgpt.py`: Interface tipo ChatGPT

---

## üéØ Projetos Especializados

### üéÆ minecraft-mcp/
**Cliente Minecraft customizado (AMCP)**

- **Finalidade**: Cliente Minecraft 1.8.9 modificado para PvP
- **Como compilar**:
  ```bash
  git clone [reposit√≥rio]
  gradlew build
  ```
- **Funcionalidades**:
  - Base Minecraft 1.8.9
  - OptiFine integrado
  - Discord Rich Presence
  - Suporte Twitch
  - Otimiza√ß√µes Windows
- **Requisitos**: Java 8, Gradle, Windows
- **‚ö†Ô∏è Aviso**: Apenas para fins educacionais

### üöÄ ideias-para-mcps/
**Plataforma MCP Cloud - Projeto Conceito**

- **Finalidade**: Plataforma tipo "Vercel para MCPs" com monetiza√ß√£o
- **Status**: Em desenvolvimento/planejamento
- **Funcionalidades planejadas**:
  - Deploy autom√°tico de servidores MCP
  - Marketplace de MCPs
  - Sistema de pagamentos DogePay
  - Infraestrutura cloud-native
  - Planos: Gratuito (R$0), Premium (R$99/m√™s), Enterprise (R$2.000/m√™s)
- **Documenta√ß√£o**: M√∫ltiplos arquivos `.md` com especifica√ß√µes detalhadas

---

## üìñ Recursos e Documenta√ß√£o

### üìù system-prompts-and-models-of-ai-tools/
**System prompts de ferramentas IA famosas**

- **Finalidade**: Cole√ß√£o de prompts internos de ferramentas como v0, Cursor, Manus
- **Conte√∫do**: Mais de 5.500 linhas de prompts oficiais
- **Pastas dispon√≠veis**:
  - v0 Folder
  - Manus Folder
  - Same.dev Folder
  - Lovable Folder
  - Devin AI Folder
  - Cursor Folder
- **Como usar**: Estude os prompts para entender como essas ferramentas funcionam

### üé® v0-system-prompts-and-models/
**Prompts espec√≠ficos do v0 da Vercel**

- **Finalidade**: Documenta√ß√£o completa dos prompts do v0
- **Conte√∫do**: 2.200+ linhas de prompts oficiais
- **Arquivos**:
  - `v0.txt`: Prompts completos do sistema
  - `v0_model.txt`: Detalhes t√©cnicos dos modelos
- **Modelos usados**:
  - Padr√£o: GPT-4o
  - Racioc√≠nio avan√ßado: DeepSeek
  - Busca futura: Sonar (Perplexity)

---

## üöÄ Como Come√ßar

### Para Desenvolvimento com IA:
1. **codex/** - Comece aqui para programa√ß√£o assistida por IA
2. **ollama_tutoriais/** - Para modelos locais
3. **openai-mcp-client/** - Para integrar ferramentas MCP

### Para Experimentar Modelos:
1. **Llama3_2_Conversational.ipynb** - Fine-tuning no Colab
2. **modelo-llama-cpp-python/** - Execu√ß√£o local
3. **modelo-bitnet-b1.58-2B-4T/** - Modelo ultra-eficiente

### Para Automa√ß√£o:
1. **github-mcp-server/** - Automa√ß√£o GitHub
2. **cryo-mcp/** - Dados blockchain
3. **chatbot-deepseek-interface/** - Interface conversacional

### Para Aprender:
1. **system-prompts-and-models-of-ai-tools/** - Como grandes ferramentas funcionam
2. **v0-system-prompts-and-models/** - Engenharia de prompt avan√ßada

---

## ‚ö†Ô∏è Avisos Importantes

- **Custos**: Ferramentas que usam APIs podem gerar custos significativos
- **Seguran√ßa**: Sempre configure adequadamente chaves de API
- **Hardware**: Alguns modelos requerem GPU ou muita RAM
- **Licen√ßas**: Verifique as licen√ßas antes de uso comercial
- **Atualiza√ß√µes**: Alguns projetos s√£o experimentais e podem mudar

---

## üÜò Suporte

Para d√∫vidas espec√≠ficas sobre cada projeto:
1. Consulte o README.md de cada pasta
2. Verifique a documenta√ß√£o oficial dos projetos
3. Procure issues nos reposit√≥rios originais
4. Para projetos locais, verifique os logs de erro

**√öltima atualiza√ß√£o**: Dezembro 2025